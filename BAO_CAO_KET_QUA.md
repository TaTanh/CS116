# B√ÅO C√ÅO K·∫æT QU·∫¢ - PRODUCT RECOMMENDATION SYSTEM

## üìä K·∫æT QU·∫¢ CU·ªêI C√ôNG

### Score tr√™n h·ªá th·ªëng th·∫ßy:
- **WITH History (X1-X13)**: **6.89%**
- **WITHOUT History (X4-X13)**: **1.35%**
- **Impact**: -80.4% khi lo·∫°i b·ªè historical features

### Metrics ƒë√°nh gi√° n·ªôi b·ªô (WITH History):
- **Precision@10**: 4.15%
- **NDCG@10**: 11.95%

### So s√°nh WITH vs WITHOUT History:

| Model | Features | Internal P@10 | Web P@10 | Impact |
|-------|----------|---------------|----------|--------|
| **WITH history** | X1-X13 | 4.15% | **6.89%** | Baseline |
| **WITHOUT history** | X4-X13 | 2.17% | **1.35%** | **-80.4%** |

‚Üí **Historical features (X1-X3) C·ª∞C K·ª≤ QUAN TR·ªåNG!**

---

## üéØ PH∆Ø∆†NG PH√ÅP

### 1. Model s·ª≠ d·ª•ng
- **Model**: LightGBM (Gradient Boosting)
- **Type**: Classification model (binary prediction)
- **Training**: ƒê√£ train tr√™n data 2024 (Jan - Nov)
- **File model**: `outputs/models/model_lightgbm_tuned_20251221_103746.pkl`

### 2. Data v√† Time Windows
**Training data:**
- Historical period: 01/01/2024 ‚Üí 01/11/2024 (11 th√°ng)
- Recent period: 01/11/2024 ‚Üí 01/12/2024 (1 th√°ng)
- Transactions: Kho·∫£ng 80M records
- Customers: 644,970 customers

**Test data (Groundtruth):**
- File: `final_groundtruth.pkl` (t·ª´ th·∫ßy)
- Customers: 644,970 (tƒÉng 253,070 so v·ªõi groundtruth c≈©)
- Format: Dictionary {customer_id: [item_ids]}

### 3. Features Engineering (13 features)
1. **X1_brand_cnt_hist**: S·ªë brand ƒë√£ mua trong l·ªãch s·ª≠
2. **X2_age_group_cnt_hist**: S·ªë age group ƒë√£ mua
3. **X3_category_cnt_hist**: S·ªë category ƒë√£ mua
4. **X4_days_since_last_purchase**: S·ªë ng√†y t·ª´ l·∫ßn mua cu·ªëi
5. **X5_purchase_frequency**: T·∫ßn su·∫•t mua h√†ng
6. **X6_is_power_user**: C√≥ ph·∫£i power user kh√¥ng
7. **X7_avg_items_per_purchase**: Trung b√¨nh items/ƒë∆°n
8. **X8_top_brand_ratio**: T·ª∑ l·ªá mua brand y√™u th√≠ch
9. **X9_brand_diversity**: ƒê·ªô ƒëa d·∫°ng brand
10. **X10_category_diversity_score**: ƒê·ªô ƒëa d·∫°ng category
11. **X11_purchase_day_mode**: Ng√†y trong tu·∫ßn hay mua
12. **X12_is_new_customer**: Kh√°ch h√†ng m·ªõi hay c≈©
13. **X13_avg_item_popularity**: ƒê·ªô ph·ªï bi·∫øn trung b√¨nh c·ªßa items

### 4. Submission Strategy
- **Customers submitted**: 100,000 customers
- **Selection method**: Top customers theo average prediction score
- **Items per customer**: 10 items (top 10 predictions)
- **File size**: 14.33 MB
- **File**: `outputs/submission_lightgbm_optimized.json`

---

## üîÑ QUY TR√åNH TH·ª∞C HI·ªÜN

### B∆∞·ªõc 1: Chu·∫©n b·ªã Data
```bash
# Convert groundtruth m·ªõi
python convert_groundtruth.py

# Convert data th√°ng 1/2025 (n·∫øu c·∫ßn train l·∫°i)
python convert_jan2025_data.py
```

### B∆∞·ªõc 2: Generate Predictions
```bash
# D√πng model ƒë√£ train s·∫µn (KH√îNG c·∫ßn train l·∫°i)
python generate_predictions_new_groundtruth.py
```
**Output:**
- Predictions file: `outputs/predictions/predictions_new_groundtruth_20251221_222506.parquet`
- Metrics file: `outputs/metrics_new_groundtruth_20251221_222506.json`
- Customers with predictions: 463,340

### B∆∞·ªõc 3: T·∫°o Submission
```bash
python optimize_submission.py
```
**Output:**
- Submission file: `outputs/submission_lightgbm_optimized.json` (14.33 MB)
- Top 100K customers, 10 items each

---

## üí° INSIGHTS V√Ä PH√ÇN T√çCH

### ƒêi·ªÉm m·∫°nh c·ªßa approach:
1. **Historical features l√† n·ªÅn t·∫£ng**
   - X1, X2, X3 ch·ª©a th√¥ng tin v·ªÅ l·ªãch s·ª≠ mua h√†ng
   - B·ªè 3 features n√†y ‚Üí gi·∫£m 80.4% performance
   - Ch·ª©ng minh: L·ªãch s·ª≠ quan tr·ªçng h∆°n h√†nh vi g·∫ßn ƒë√¢y

2. **Kh√¥ng c·∫ßn train l·∫°i model**
   - Ti·∫øt ki·ªám th·ªùi gian (5-10 ph√∫t vs 1-2 gi·ªù)
   - Ti·∫øt ki·ªám RAM
   - Model c≈© (train tr√™n 11 th√°ng 2024) v·∫´n r·∫•t t·ªët

3. **Ch·ªçn l·ªçc customers th√¥ng minh**
   - Ch·ªâ submit top 100K customers c√≥ score cao nh·∫•t
   - TƒÉng precision (focus v√†o predictions t·ªët nh·∫•t)
   - Gi·∫£m file size (d·ªÖ upload, d·ªÖ x·ª≠ l√Ω)

4. **Feature engineering ƒëa d·∫°ng**
   - K·∫øt h·ª£p features v·ªÅ behavior (purchase frequency, recency)
   - Features v·ªÅ preferences (brand, category diversity)
   - Features v·ªÅ popularity (item popularity)

### So s√°nh v·ªõi baseline:
- **Groundtruth c≈©**: 391,900 customers
- **Groundtruth m·ªõi**: 644,970 customers (+65% customers)
- **Coverage**: 463,340 / 644,970 = 71.8% customers c√≥ predictions

### K·∫øt qu·∫£:
- **Public score WITH history**: 6.89%
- **Public score WITHOUT history**: 1.35%
- **T·ªët h∆°n random baseline** (< 1%)
- **Precision@10 internal**: 4.15% (WITH history) vs 2.17% (WITHOUT history)

---

## üìÅ C·∫§U TR√öC FILES QUAN TR·ªåNG

### Input Files:
- `groundtruth.pkl` - Test set (644,970 customers)
- `final_groundtruth.pkl` - Groundtruth g·ªëc t·ª´ th·∫ßy
- `01-2025.pkl` - Data th√°ng 1/2025 (n·∫øu c·∫ßn)

### Model Files:
- `outputs/models/model_lightgbm_tuned_20251221_103746.pkl` - Best model

### Output Files:
- `outputs/predictions/predictions_new_groundtruth_20251221_222506.parquet` - Predictions
- `outputs/metrics_new_groundtruth_20251221_222506.json` - Metrics
- `outputs/submission_lightgbm_optimized.json` - **SUBMISSION FILE** (14.33 MB)

### Scripts:
- `convert_groundtruth.py` - Convert groundtruth format
- `generate_predictions_new_groundtruth.py` - Generate predictions
- `optimize_submission.py` - Create submission file

---

## üéì K·∫æT LU·∫¨N

### Th√†nh c√¥ng:
- ‚úÖ X√¢y d·ª±ng h·ªá th·ªëng recommendation ho√†n ch·ªânh
- ‚úÖ ƒê·∫°t score 6.89% tr√™n public test set
- ‚úÖ X·ª≠ l√Ω 644K+ customers, 80M+ transactions
- ‚úÖ Approach th·ª±c t·∫ø, t·ªëi ∆∞u (kh√¥ng c·∫ßn train l·∫°i)

### B√†i h·ªçc:
1. **Feature engineering quan tr·ªçng h∆°n model ph·ª©c t·∫°p**
2. **Ch·ªçn l·ªçc customers th√¥ng minh** (top score) tƒÉng precision
3. **Model ƒë∆°n gi·∫£n nh∆∞ng t·ªët** c√≥ th·ªÉ d√πng l·∫°i cho test set m·ªõi

### C·∫£i thi·ªán c√≥ th·ªÉ:
- Ensemble multiple models (LightGBM + XGBoost + Random Forest)
- Th√™m features v·ªÅ item characteristics
- Hyperparameter tuning k·ªπ h∆°n
- TƒÉng s·ªë customers submit (n·∫øu server cho ph√©p)

---

**Ng√†y ho√†n th√†nh**: 21/12/2025
**Model**: LightGBM Tuned
**Final Score**: 6.89%
