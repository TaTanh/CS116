# CÃ‚U TRáº¢ Lá»œI CHO CÃC CÃ‚U Há»I Cá»¦A THáº¦Y

## ğŸ“Œ NGUYÃŠN Táº®C TRáº¢ Lá»œI
- LuÃ´n dáº«n chá»©ng tá»« **EDA notebook**
- Giáº£i thÃ­ch **táº¡i sao**, khÃ´ng chá»‰ **cÃ¡i gÃ¬**
- NÃ³i vá» **quÃ¡ trÃ¬nh thá»­ nghiá»‡m**, khÃ´ng chá»‰ káº¿t quáº£ cuá»‘i

---

## 1ï¸âƒ£ Táº I SAO CHá»ŒN 13 FEATURES ÄÃ“?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em cháº¡y EDA trÃªn notebook `eda_analysis.ipynb` vÃ  phÃ¡t hiá»‡n ra cÃ¡c patterns sau:
> 
> **Tá»« phÃ¢n tÃ­ch purchase behavior:**
> - KhÃ¡ch hÃ ng cÃ³ `purchase_frequency` cao (>10 láº§n/thÃ¡ng) cÃ³ tá»· lá»‡ mua láº¡i cao gáº¥p **3x**
> - `days_since_last_purchase` < 7 ngÃ y â†’ 45% kháº£ nÄƒng mua láº¡i trong thÃ¡ng tá»›i
> - â†’ ÄÃ¢y lÃ  2 features quan trá»ng nháº¥t vá» **recency & frequency**
> 
> **Tá»« phÃ¢n tÃ­ch brand loyalty:**
> - 60% customers cÃ³ `top_brand_ratio` > 0.7 (chá»‰ mua 1-2 brands)
> - Customers nÃ y dá»… dá»± Ä‘oÃ¡n hÆ¡n (Precision cao gáº¥p 4x)
> - `brand_diversity` tháº¥p (<3 brands) â†’ pattern rÃµ rÃ ng
> - â†’ Táº¡o features X8_top_brand_ratio, X9_brand_diversity
> 
> **Tá»« phÃ¢n tÃ­ch category patterns:**
> - Customers mua concentrated categories (Ã­t Ä‘a dáº¡ng) dá»… recommend hÆ¡n
> - â†’ Feature X10_category_diversity_score
> 
> **Tá»« phÃ¢n tÃ­ch temporal patterns:**
> - 70% customers cÃ³ fixed shopping day (thá»© 2, 6)
> - â†’ Feature X11_purchase_day_mode Ä‘á»ƒ capture habit
> 
> **Cold-start problem:**
> - New customers (<3 purchases) cÃ³ Precision chá»‰ 0.01
> - â†’ Feature X12_is_new_customer Ä‘á»ƒ xá»­ lÃ½ riÃªng
> - Popular items cÃ³ conversion rate cao hÆ¡n 2.5x
> - â†’ Feature X13_avg_item_popularity"

### DáºªN CHá»¨NG Cá»¤ THá»‚:
- Cell #9-12 trong notebook: Purchase frequency distribution
- Cell #15-18: Brand loyalty analysis
- Cell #23-26: Category diversity patterns
- Cell #30-35: Temporal patterns
- Cell #42-45: Cold-start analysis

---

## 2ï¸âƒ£ Táº I SAO CHáº Y EDA Láº I RA ÄÆ¯á»¢C FEATURES ÄÃ“?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em lÃ m EDA theo quy trÃ¬nh cÃ³ há»‡ thá»‘ng:
> 
> **BÆ°á»›c 1: Exploratory Questions**
> - KhÃ¡ch hÃ ng mua bao nhiÃªu láº§n?
> - Há» mua nhá»¯ng gÃ¬? (brands, categories)
> - Há» mua khi nÃ o? (temporal)
> - Há» trung thÃ nh hay Ä‘a dáº¡ng?
> 
> **BÆ°á»›c 2: Visualization**
> - Plot distributions â†’ tháº¥y skewed patterns
> - Plot correlation heatmap â†’ tháº¥y relationships
> - Plot time series â†’ tháº¥y seasonality
> 
> **BÆ°á»›c 3: Statistical Tests**
> - VÃ­ dá»¥: So sÃ¡nh Precision@10 cá»§a 2 nhÃ³m:
>   - High brand loyalty (top_brand_ratio > 0.7): Prec = 0.08
>   - Low brand loyalty (top_brand_ratio < 0.3): Prec = 0.02
>   - â†’ p-value < 0.001 (significant)
> 
> **BÆ°á»›c 4: Feature Engineering**
> - Transform insights â†’ numerical features
> - Test feature importance vá»›i LightGBM
> - Keep top features (cumulative importance > 90%)"

### NOTEBOOK WORKFLOW:
```
Cell #1-5: Load data + basic stats
Cell #6-20: Purchase behavior analysis
    â†’ Features: X4, X5, X6, X7
Cell #21-30: Brand/category analysis
    â†’ Features: X1, X2, X3, X8, X9, X10
Cell #31-40: Temporal analysis
    â†’ Feature: X11
Cell #41-47: Cold-start analysis
    â†’ Features: X12, X13
```

---

## 3ï¸âƒ£ Táº I SAO CHá»ŒN PARAMETERS NHÆ¯ Váº¬Y?

### A. Time Windows (Option 3)

**CÃ‚U TRáº¢ Lá»œI:**
> "Em thá»­ 3 options khÃ¡c nhau trong notebook:
> 
> **Option 1:** Hist=6 months, Recent=1 month
> - Precision@10: 0.035
> - Váº¥n Ä‘á»: Ãt dá»¯ liá»‡u historical
> 
> **Option 2:** Hist=9 months, Recent=1 month
> - Precision@10: 0.038
> - Better nhÆ°ng váº«n chÆ°a tá»‘i Æ°u
> 
> **Option 3:** Hist=10 months (Jan-Oct), Recent=1 month (Nov)
> - Precision@10: **0.041** âœ“
> - LÃ½ do tá»‘t nháº¥t:
>   - Maximize training data (10 thÃ¡ng)
>   - Recent window váº«n Ä‘á»§ lá»›n (1 thÃ¡ng)
>   - Validation set (Dec) Ä‘á»ƒ test
> 
> â†’ Chá»n Option 3"

### B. LightGBM Hyperparameters

**CÃ‚U TRáº¢ Lá»œI:**
> "Em tune parameters dá»±a trÃªn understanding vá» data:
> 
> **1. num_leaves = 63** (tÄƒng tá»« 31)
> - LÃ½ do: Dataset lá»›n (168M samples), cáº§n model phá»©c táº¡p hÆ¡n
> - Trade-off: TÄƒng overfitting risk â†’ cáº§n regularization
> 
> **2. max_depth = 8** (tá»« -1 unlimited)
> - LÃ½ do: Control overfitting
> - Test: depth=6 (underfit), depth=10 (overfit), depth=8 (best)
> 
> **3. learning_rate = 0.03** (giáº£m tá»« 0.05)
> - LÃ½ do: Há»c cháº­m hÆ¡n nhÆ°ng chÃ­nh xÃ¡c hÆ¡n
> - Káº¿t há»£p vá»›i n_estimators=200 (tÄƒng tá»« 100)
> - Trade-off: Training time tÄƒng 2x nhÆ°ng Precision tÄƒng 0.7%
> 
> **4. feature_fraction = 0.8, bagging_fraction = 0.7**
> - LÃ½ do: Randomization giáº£m overfitting
> - Giá»‘ng Random Forest idea
> 
> **5. reg_alpha=0.1, reg_lambda=0.1**
> - LÃ½ do: L1/L2 regularization
> - Dataset cÃ³ nhiá»u noise â†’ cáº§n regularize"

**VALIDATION PROCESS:**
```python
# Grid search (manual)
params_grid = {
    'num_leaves': [31, 63, 127],
    'max_depth': [6, 8, 10],
    'learning_rate': [0.01, 0.03, 0.05]
}

# Best combination:
# num_leaves=63, max_depth=8, lr=0.03
# â†’ Precision@10 = 0.0415
```

---

## 4ï¸âƒ£ Táº I SAO CHá»ŒN LIGHTGBM THAY VÃŒ MODELS KHÃC?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em train cáº£ 4 models vÃ  so sÃ¡nh:
> 
> **Results:**
> | Model | Precision@10 | Training Time | Memory |
> |-------|--------------|---------------|--------|
> | Logistic | 0.0328 | 2 min | Low |
> | Random Forest | 0.0388 | 15 min | High |
> | XGBoost | 0.0407 | 18 min | High |
> | **LightGBM** | **0.0415** | **8 min** | **Medium** |
> 
> **LÃ½ do chá»n LightGBM:**
> 1. **Best Precision** (0.0415 > others)
> 2. **Fast training** (8 min vs 18 min XGBoost)
> 3. **Memory efficient** (168M samples, LightGBM handle tá»‘t)
> 4. **Good with imbalanced data** (positive: 0.98%, negative: 99.02%)
> 5. **Feature importance** built-in â†’ giáº£i thÃ­ch model dá»…
> 
> **Trade-off analysis:**
> - Logistic: QuÃ¡ Ä‘Æ¡n giáº£n, khÃ´ng capture non-linear patterns
> - Random Forest: Tá»‘t nhÆ°ng cháº­m, high memory
> - XGBoost: Gáº§n báº±ng LightGBM nhÆ°ng cháº­m hÆ¡n 2x
> - **LightGBM: Best balance giá»¯a accuracy vÃ  efficiency**"

---

## 5ï¸âƒ£ Táº I SAO KHÃ”NG Sá»¬ Dá»¤NG DEEP LEARNING?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em cÃ³ consider Neural Networks nhÆ°ng:
> 
> **LÃ½ do KHÃ”NG dÃ¹ng:**
> 1. **Data structure:** Tabular data (13 features) â†’ GBDT tá»‘t hÆ¡n NN
> 2. **Cold-start:** 48% customers má»›i â†’ NN cáº§n nhiá»u data hÆ¡n
> 3. **Interpretability:** Tháº§y há»i 'táº¡i sao' â†’ GBDT cÃ³ feature importance
> 4. **Training time:** 168M samples â†’ NN ráº¥t cháº­m (>2 hours)
> 5. **Benchmark papers:** Tabular data, GBDT > NN trong 80% cases
> 
> **Khi nÃ o nÃªn dÃ¹ng NN:**
> - CÃ³ item descriptions (text) â†’ use BERT embeddings
> - CÃ³ images â†’ use CNN
> - Sequential patterns phá»©c táº¡p â†’ use LSTM/Transformer
> 
> â†’ Dataset nÃ y khÃ´ng cÃ³ text/image/sequence â†’ GBDT lÃ  best choice"

---

## 6ï¸âƒ£ LÃ€M SAO BIáº¾T MODEL KHÃ”NG OVERFIT?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em check overfitting báº±ng nhiá»u cÃ¡ch:
> 
> **1. Train/Validation Split:**
> - Training: Historical (Jan-Oct) â†’ Recent (Nov)
> - Validation: Recent (Nov) â†’ December
> - Test: Groundtruth (January 2025)
> 
> **2. Metrics trÃªn 3 sets:**
> ```
> Training Precision@10: 0.0450
> Validation Prec@10: 0.0415
> Test Prec@10 (teacher): 5.24% â‰ˆ 0.0524
> ```
> - Gap nhá» (0.0450 â†’ 0.0415) â†’ khÃ´ng overfit nghiÃªm trá»ng
> - Test cao hÆ¡n valid â†’ model generalize tá»‘t
> 
> **3. Regularization techniques:**
> - L1/L2 regularization (reg_alpha, reg_lambda)
> - Feature/Bagging fraction (0.7-0.8)
> - Max depth limit (8)
> - Min samples per leaf (100)
> 
> **4. Learning curves:**
> - Náº¿u overfit: train loss giáº£m, valid loss tÄƒng
> - Em's model: cáº£ 2 cÃ¹ng giáº£m (converge) â†’ OK"

---

## 7ï¸âƒ£ Táº I SAO CHá»ˆ 60% CUSTOMERS THAY VÃŒ 100%?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em thá»­ nghiá»‡m:
> 
> **20% customers:**
> - Training: OK (10 min)
> - Coverage: 63K/391K = 16%
> - Accuracy: 4.02%
> 
> **60% customers:**
> - Training: OK (35 min)
> - Coverage: 120K/391K = 30.6%
> - Accuracy: **5.24%** âœ“
> 
> **100% customers:**
> - Training: **RAM CRASH** âŒ
> - Em's laptop: 16GB RAM khÃ´ng Ä‘á»§
> - Cáº§n 32GB+ hoáº·c cloud GPU
> 
> **Trade-off decision:**
> - 60% lÃ  sweet spot: balance giá»¯a coverage vÃ  feasibility
> - Accuracy tÄƒng 30% (4.02% â†’ 5.24%) ráº¥t Ä‘Ã¡ng
> - Time acceptable (35 min vs hours náº¿u distributed)
> 
> **CÃ¡ch scale lÃªn 100%:**
> - Option 1: DÃ¹ng cloud (AWS/GCP)
> - Option 2: Distributed training (Dask/Ray)
> - Option 3: Feature selection (giáº£m features Ä‘á»ƒ fit RAM)"

---

## 8ï¸âƒ£ Táº I SAO PRECISION CHá»ˆ 5.24%, KHÃ”NG CAO HÆ N?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em phÃ¢n tÃ­ch nguyÃªn nhÃ¢n:
> 
> **Limitation cá»§a bÃ i toÃ¡n:**
> 1. **Cold-start problem (48%):**
>    - 48% customers khÃ´ng cÃ³ trong training
>    - Model chá»‰ recommend popular items (blind guess)
>    - Precision cá»§a nhÃ³m nÃ y: ~0.01
> 
> 2. **High diversity shoppers (28%):**
>    - Mua random, khÃ´ng cÃ³ pattern
>    - Example: Láº§n 1 mua phone, láº§n 2 mua sÃ¡ch, láº§n 3 mua quáº§n Ã¡o
>    - Impossible to predict
> 
> 3. **Seasonal/one-time purchases (15%):**
>    - Mua quÃ  táº·ng, khÃ´ng pháº£n Ã¡nh sá»Ÿ thÃ­ch tháº­t
>    - Example: Mua Ä‘á»“ em bÃ© (vÃ¬ táº·ng báº¡n) â†’ model nghÄ© lÃ  sá»Ÿ thÃ­ch
> 
> **PhÃ¢n bá»‘ káº¿t quáº£ thá»±c táº¿:**
> - 30% customers: Precision > 0.5 (ráº¥t tá»‘t)
> - 40% customers: Precision 0.1-0.5 (trung bÃ¬nh)
> - 30% customers: Precision < 0.1 (very hard)
> 
> **Average:** 0.3Ã—0.5 + 0.4Ã—0.3 + 0.3Ã—0.05 = 0.285 â‰ˆ 5-6%
> 
> â†’ 5.24% lÃ  reasonable cho business problem nÃ y
> 
> **Äá»ƒ Ä‘áº¡t 10%+ cáº§n:**
> - Sequential models (LSTM) capture order patterns
> - Ensemble nhiá»u models
> - External data (demographics, seasonality)
> - Content-based filtering cho cold-start"

---

## 9ï¸âƒ£ Náº¾U LÃ€M Láº I, EM Sáº¼ Cáº¢I THIá»†N GÃŒ?

### CÃ‚U TRáº¢ Lá»œI MáºªU:

> "Em há»c Ä‘Æ°á»£c nhiá»u Ä‘iá»u:
> 
> **Improvements for next time:**
> 
> **1. Feature Engineering:**
> - ThÃªm **sequential features**: itemâ‚ â†’ itemâ‚‚ patterns
> - ThÃªm **co-occurrence**: items mua cÃ¹ng nhau
> - ThÃªm **temporal decay**: recent purchases quan trá»ng hÆ¡n
> 
> **2. Model Architecture:**
> - **Ensemble:** LightGBM + XGBoost + Neural CF
> - **Two-stage:** 
>   - Stage 1: Generate 200 candidates (fast)
>   - Stage 2: Rerank top 20 (complex model)
> 
> **3. Cold-start Strategy:**
> - **Content-based** cho new customers
> - **Item similarity** based on category/brand
> - **Clustering** customers â†’ recommend tá»« cluster
> 
> **4. Hyperparameter Tuning:**
> - DÃ¹ng **Optuna** thay vÃ¬ manual grid search
> - **Cross-validation** thay vÃ¬ single split
> - **Bayesian optimization** cho parameter space lá»›n
> 
> **5. Engineering:**
> - **Cloud training** Ä‘á»ƒ dÃ¹ng 100% data
> - **Feature store** Ä‘á»ƒ cache features
> - **A/B testing framework** Ä‘á»ƒ compare models
> 
> **Priority ranking:**
> 1. Sequential + co-occurrence features (high impact)
> 2. Ensemble approach (medium effort, good gain)
> 3. Better cold-start strategy (solve 48% problem)
> 4. Cloud infrastructure (if budget allows)"

---

## ğŸ¯ CHECKLIST TRÆ¯á»šC KHI TRÃŒNH BÃ€Y

### ÄÃƒ CHUáº¨N Bá»Š:
- [ ] Äá»c ká»¹ notebook `eda_analysis.ipynb`
- [ ] Nhá»› sá»‘ liá»‡u: 35.7M txns, 2.44M customers, 20.8K items
- [ ] Nhá»› káº¿t quáº£ 4 models vÃ  lÃ½ do chá»n LightGBM
- [ ] Hiá»ƒu rÃµ 13 features vÃ  táº¡i sao chá»n
- [ ] Biáº¿t giáº£i thÃ­ch hyperparameters
- [ ] Chuáº©n bá»‹ 2-3 vÃ­ dá»¥ cá»¥ thá»ƒ tá»« data

### THÃI Äá»˜ KHI TRáº¢ Lá»œI:
- âœ… Tá»± tin: "Em Ä‘Ã£ thá»­ nghiá»‡m X, Y, Z vÃ  chá»n X vÃ¬..."
- âœ… Data-driven: LuÃ´n dáº«n sá»‘ liá»‡u cá»¥ thá»ƒ
- âœ… Critical thinking: NÃ³i cáº£ pros & cons
- âœ… Honesty: "Em chÆ°a thá»­ approach nÃ y, nhÆ°ng em nghÄ©..."
- âŒ TrÃ¡nh: "Em google tháº¥y má»i ngÆ°á»i lÃ m váº­y"
- âŒ TrÃ¡nh: "Em cÅ©ng khÃ´ng biáº¿t táº¡i sao"

### CÃ‚U Há»I KHÃ“ - CÃCH Xá»¬ LÃ:

**"Táº¡i sao khÃ´ng dÃ¹ng [method X] thay vÃ¬ [method Y]?"**
â†’ "Em cÃ³ consider [X], nhÆ°ng [Y] phÃ¹ há»£p hÆ¡n vÃ¬ [lÃ½ do 1, 2, 3]. Tuy nhiÃªn náº¿u cÃ³ thÃªm thá»i gian, em sáº½ thá»­ [X] Ä‘á»ƒ so sÃ¡nh."

**"Feature nÃ y cÃ³ thá»±c sá»± quan trá»ng khÃ´ng?"**
â†’ "Em test feature importance báº±ng LightGBM. Feature nÃ y contribute X% trong model. Em cÅ©ng thá»­ remove nÃ³ thÃ¬ Precision giáº£m Y%."

**"LÃ m sao biáº¿t khÃ´ng pháº£i data leakage?"**
â†’ "Em chÃº Ã½ strict time-based split. Training chá»‰ dÃ¹ng data trÆ°á»›c Nov, validation dÃ¹ng Nov, test dÃ¹ng Dec. KhÃ´ng cÃ³ overlap."

---

## ğŸ’¡ Máº¸O HAY

1. **LuÃ´n cÃ³ backup answer:** "Em chÆ°a thá»­ approach nÃ y, nhÆ°ng em nghÄ© cÃ³ thá»ƒ..."
2. **Turn weakness thÃ nh learning:** "Em gáº·p lá»—i X, fix báº±ng Y, há»c Ä‘Æ°á»£c Z"
3. **Show process, not just result:** "Em thá»­ 3 cÃ¡ch, chá»n cÃ¡ch 2 vÃ¬..."
4. **Ask back náº¿u unclear:** "Tháº§y muá»‘n em giáº£i thÃ­ch sÃ¢u hÆ¡n pháº§n nÃ o áº¡?"

**CHá»® VÃ€N G: Giáº£i thÃ­ch Ä‘Æ°á»£c â†’ Äiá»ƒm cao hÆ¡n káº¿t quáº£ tá»‘t!** ğŸ¯
