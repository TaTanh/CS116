{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57af221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(20)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f32cbe",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c5e225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from chunks...\n",
      "This may take a moment as we're loading ~36M transactions...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "The system cannot find the file specified. (os error 2): E:\\Nam_3_HK1\\PythonMayHoc\\dataset\\transactions.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m users_lazy = load_users()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Collect to DataFrame (execute the query)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m transactions = \u001b[43mtransactions_lazy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m items = items_lazy.collect()\n\u001b[32m     18\u001b[39m users = users_lazy.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2415\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2413\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2414\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The system cannot find the file specified. (os error 2): E:\\Nam_3_HK1\\PythonMayHoc\\dataset\\transactions.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "# Load data using recommender functions (auto-loads all chunks)\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.recommender import load_transactions, load_items, load_users\n",
    "\n",
    "print(\"Loading data from chunks...\")\n",
    "print(\"This may take a moment as we're loading ~36M transactions...\")\n",
    "\n",
    "# Load all data (LazyFrame - not yet executed)\n",
    "transactions_lazy = load_transactions()\n",
    "items_lazy = load_items()\n",
    "users_lazy = load_users()\n",
    "\n",
    "# Collect to DataFrame (execute the query)\n",
    "transactions = transactions_lazy.collect()\n",
    "items = items_lazy.collect()\n",
    "users = users_lazy.collect()\n",
    "\n",
    "print(\"\\nData loaded successfully!\")\n",
    "print(f\"Transactions: {transactions.shape}\")\n",
    "print(f\"Items: {items.shape}\")\n",
    "print(f\"Users: {users.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick schema overview\n",
    "print(\"\\n=== TRANSACTIONS SCHEMA ===\")\n",
    "print(transactions.schema)\n",
    "print(\"\\nSample:\")\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ITEMS SCHEMA ===\")\n",
    "print(items.schema)\n",
    "print(\"\\nSample:\")\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== USERS SCHEMA ===\")\n",
    "print(users.schema)\n",
    "print(\"\\nSample:\")\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fb405",
   "metadata": {},
   "source": [
    "## 2. Task 1: Univariate Analysis\n",
    "\n",
    "### 2.1 Basic Statistics & Null Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_null_rates(df: pl.DataFrame, name: str) -> pl.DataFrame:\n",
    "    \"\"\"Compute null rates for all columns.\"\"\"\n",
    "    total = df.shape[0]\n",
    "    \n",
    "    null_counts = df.select([\n",
    "        pl.col(col).is_null().sum().alias(col) \n",
    "        for col in df.columns\n",
    "    ])\n",
    "    \n",
    "    null_rates = pl.DataFrame({\n",
    "        'column': df.columns,\n",
    "        'null_count': null_counts.row(0),\n",
    "        'null_rate': [n / total for n in null_counts.row(0)],\n",
    "    }).sort('null_rate', descending=True)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"NULL RATES - {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(null_rates)\n",
    "    \n",
    "    return null_rates\n",
    "\n",
    "# Compute null rates for all datasets\n",
    "null_txns = compute_null_rates(transactions, 'TRANSACTIONS')\n",
    "null_items = compute_null_rates(items, 'ITEMS')\n",
    "null_users = compute_null_rates(users, 'USERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize null rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (df_null, title) in zip(axes, [\n",
    "    (null_txns, 'Transactions'),\n",
    "    (null_items, 'Items'),\n",
    "    (null_users, 'Users')\n",
    "]):\n",
    "    if df_null.shape[0] > 0 and df_null['null_rate'].max() > 0:\n",
    "        ax.barh(df_null['column'].to_list(), df_null['null_rate'].to_list())\n",
    "        ax.set_xlabel('Null Rate')\n",
    "        ax.set_title(f'{title} - Null Rates')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No nulls', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'{title} - Null Rates')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd57c4a",
   "metadata": {},
   "source": [
    "### 2.2 Customer Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer-level statistics\n",
    "customer_stats = transactions.group_by('customer_id').agg([\n",
    "    pl.count('item_id').alias('num_purchases'),\n",
    "    pl.n_unique('item_id').alias('num_unique_items'),\n",
    "    pl.n_unique('order_id').alias('num_orders'),\n",
    "    (pl.col('created_at').max() - pl.col('created_at').min()).dt.total_days().alias('days_active'),\n",
    "])\n",
    "\n",
    "print(\"\\nCustomer Statistics:\")\n",
    "print(customer_stats.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot customer distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Number of purchases per customer\n",
    "axes[0, 0].hist(customer_stats['num_purchases'].to_numpy(), bins=50, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Number of Purchases')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution: Purchases per Customer')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Unique items per customer\n",
    "axes[0, 1].hist(customer_stats['num_unique_items'].to_numpy(), bins=50, edgecolor='black', color='green')\n",
    "axes[0, 1].set_xlabel('Number of Unique Items')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution: Unique Items per Customer')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Orders per customer\n",
    "axes[1, 0].hist(customer_stats['num_orders'].to_numpy(), bins=50, edgecolor='black', color='orange')\n",
    "axes[1, 0].set_xlabel('Number of Orders')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution: Orders per Customer')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Days active\n",
    "axes[1, 1].hist(customer_stats['days_active'].to_numpy(), bins=50, edgecolor='black', color='red')\n",
    "axes[1, 1].set_xlabel('Days Active')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution: Customer Activity Duration')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd470740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top customers by purchase volume\n",
    "top_customers = customer_stats.sort('num_purchases', descending=True).head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_customers)), top_customers['num_purchases'].to_list())\n",
    "plt.xlabel('Number of Purchases')\n",
    "plt.ylabel('Customer Rank')\n",
    "plt.title('Top 20 Customers by Purchase Volume')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb840305",
   "metadata": {},
   "source": [
    "### 2.3 Item Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ff31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item popularity\n",
    "item_stats = transactions.group_by('item_id').agg([\n",
    "    pl.count().alias('num_purchases'),\n",
    "    pl.n_unique('customer_id').alias('num_customers'),\n",
    "]).join(items, on='item_id', how='left')\n",
    "\n",
    "print(\"\\nItem Statistics:\")\n",
    "print(item_stats.select(['num_purchases', 'num_customers']).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot item distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Purchases per item\n",
    "axes[0].hist(item_stats['num_purchases'].to_numpy(), bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Number of Purchases')\n",
    "axes[0].set_ylabel('Frequency (Items)')\n",
    "axes[0].set_title('Distribution: Purchases per Item')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Customers per item\n",
    "axes[1].hist(item_stats['num_customers'].to_numpy(), bins=50, edgecolor='black', color='purple')\n",
    "axes[1].set_xlabel('Number of Unique Customers')\n",
    "axes[1].set_ylabel('Frequency (Items)')\n",
    "axes[1].set_title('Distribution: Customers per Item')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d34232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top items by popularity\n",
    "top_items = item_stats.sort('num_purchases', descending=True).head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_items)), top_items['num_purchases'].to_list())\n",
    "plt.xlabel('Number of Purchases')\n",
    "plt.ylabel('Item Rank')\n",
    "plt.title('Top 20 Most Popular Items')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3cf40",
   "metadata": {},
   "source": [
    "### 2.4 Categorical Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e39b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand distribution\n",
    "brand_dist = items.group_by('brand').agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "\n",
    "print(\"\\nTop 10 Brands:\")\n",
    "print(brand_dist.head(10))\n",
    "\n",
    "# Plot top 15 brands\n",
    "top_brands = brand_dist.head(15)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(top_brands)), top_brands['count'].to_list())\n",
    "plt.xticks(range(len(top_brands)), top_brands['brand'].to_list(), rotation=45, ha='right')\n",
    "plt.xlabel('Brand')\n",
    "plt.ylabel('Number of Items')\n",
    "plt.title('Top 15 Brands by Item Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age group distribution\n",
    "age_group_dist = items.group_by('age_group').agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "print(age_group_dist)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(age_group_dist['age_group'].to_list(), age_group_dist['count'].to_list())\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Items')\n",
    "plt.title('Distribution: Age Groups')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "category_dist = items.group_by('category').agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "\n",
    "print(\"\\nTop 10 Categories:\")\n",
    "print(category_dist.head(10))\n",
    "\n",
    "# Plot top 15 categories\n",
    "top_categories = category_dist.head(15)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(top_categories)), top_categories['count'].to_list(), color='coral')\n",
    "plt.xticks(range(len(top_categories)), top_categories['category'].to_list(), rotation=45, ha='right')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Items')\n",
    "plt.title('Top 15 Categories by Item Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6254e2",
   "metadata": {},
   "source": [
    "### 2.5 Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction time range\n",
    "time_range = transactions.select([\n",
    "    pl.col('created_at').min().alias('first_transaction'),\n",
    "    pl.col('created_at').max().alias('last_transaction'),\n",
    "])\n",
    "\n",
    "print(\"\\nTransaction Time Range:\")\n",
    "print(time_range)\n",
    "\n",
    "# Daily transaction volume\n",
    "daily_txns = transactions.group_by(\n",
    "    pl.col('created_at').dt.date().alias('date')\n",
    ").agg(\n",
    "    pl.count().alias('num_transactions')\n",
    ").sort('date')\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(daily_txns['date'].to_list(), daily_txns['num_transactions'].to_list())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.title('Daily Transaction Volume Over Time')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e34da3",
   "metadata": {},
   "source": [
    "## 3. Task 2: Multivariate Analysis\n",
    "\n",
    "### 3.1 Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interaction matrix sparsity\n",
    "num_customers = transactions['customer_id'].n_unique()\n",
    "num_items = transactions['item_id'].n_unique()\n",
    "num_interactions = transactions.shape[0]\n",
    "total_possible = num_customers * num_items\n",
    "sparsity = 1 - (num_interactions / total_possible)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"INTERACTION MATRIX SPARSITY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Unique customers: {num_customers:,}\")\n",
    "print(f\"Unique items: {num_items:,}\")\n",
    "print(f\"Total interactions: {num_interactions:,}\")\n",
    "print(f\"Possible interactions: {total_possible:,}\")\n",
    "print(f\"Sparsity: {sparsity:.4%}\")\n",
    "print(f\"Density: {(1-sparsity):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sparsity\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "categories = ['Filled', 'Empty']\n",
    "values = [1-sparsity, sparsity]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "ax.pie(values, labels=categories, autopct='%1.2f%%', colors=colors, startangle=90)\n",
    "ax.set_title('Customer-Item Interaction Matrix Sparsity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2cd4f",
   "metadata": {},
   "source": [
    "### 3.2 Purchase Patterns by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join transactions with items to analyze category purchases\n",
    "txns_with_items = transactions.join(items, on='item_id', how='left')\n",
    "\n",
    "# Category purchase volume\n",
    "category_purchases = txns_with_items.group_by('category').agg(\n",
    "    pl.count().alias('num_purchases')\n",
    ").sort('num_purchases', descending=True)\n",
    "\n",
    "print(\"\\nTop Categories by Purchase Volume:\")\n",
    "print(category_purchases.head(10))\n",
    "\n",
    "# Plot\n",
    "top_cat = category_purchases.head(15)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_cat)), top_cat['num_purchases'].to_list())\n",
    "plt.yticks(range(len(top_cat)), top_cat['category'].to_list())\n",
    "plt.xlabel('Number of Purchases')\n",
    "plt.title('Top 15 Categories by Purchase Volume')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc785b",
   "metadata": {},
   "source": [
    "### 3.3 Brand vs Age Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93524fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation: Brand vs Age Group\n",
    "brand_age = items.group_by(['brand', 'age_group']).agg(\n",
    "    pl.count().alias('item_count')\n",
    ")\n",
    "\n",
    "# Get top 10 brands\n",
    "top_10_brands = brand_dist.head(10)['brand'].to_list()\n",
    "\n",
    "# Filter for top brands and pivot\n",
    "brand_age_pivot = brand_age.filter(\n",
    "    pl.col('brand').is_in(top_10_brands)\n",
    ").pivot(\n",
    "    index='brand',\n",
    "    columns='age_group',\n",
    "    values='item_count',\n",
    "    aggregate_function='sum'\n",
    ").fill_null(0)\n",
    "\n",
    "print(\"\\nBrand vs Age Group Matrix (Top 10 Brands):\")\n",
    "print(brand_age_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap-style visualization\n",
    "pivot_np = brand_age_pivot.select(pl.exclude('brand')).to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "im = ax.imshow(pivot_np, aspect='auto', cmap='YlOrRd')\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(range(len(brand_age_pivot.columns[1:])))\n",
    "ax.set_yticks(range(len(brand_age_pivot)))\n",
    "ax.set_xticklabels(brand_age_pivot.columns[1:], rotation=45, ha='right')\n",
    "ax.set_yticklabels(brand_age_pivot['brand'].to_list())\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=ax, label='Item Count')\n",
    "\n",
    "ax.set_title('Brand vs Age Group Heatmap (Top 10 Brands)')\n",
    "ax.set_xlabel('Age Group')\n",
    "ax.set_ylabel('Brand')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f816a",
   "metadata": {},
   "source": [
    "### 3.4 Cohort-Like Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer first purchase month\n",
    "customer_first_purchase = transactions.group_by('customer_id').agg(\n",
    "    pl.col('created_at').min().alias('first_purchase')\n",
    ").with_columns(\n",
    "    pl.col('first_purchase').dt.month_start().alias('cohort_month')\n",
    ")\n",
    "\n",
    "# Join back to transactions\n",
    "cohort_txns = transactions.join(\n",
    "    customer_first_purchase.select(['customer_id', 'cohort_month']),\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ").with_columns(\n",
    "    pl.col('created_at').dt.month_start().alias('purchase_month')\n",
    ")\n",
    "\n",
    "# Calculate months since first purchase\n",
    "cohort_analysis = cohort_txns.with_columns(\n",
    "    ((pl.col('purchase_month') - pl.col('cohort_month')).dt.total_days() / 30).floor().cast(pl.Int32).alias('months_since_first')\n",
    ").group_by(['cohort_month', 'months_since_first']).agg(\n",
    "    pl.n_unique('customer_id').alias('active_customers')\n",
    ").sort(['cohort_month', 'months_since_first'])\n",
    "\n",
    "print(\"\\nCohort Analysis (Sample):\")\n",
    "print(cohort_analysis.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot retention by cohort\n",
    "# Select a few cohorts for visualization\n",
    "sample_cohorts = cohort_analysis['cohort_month'].unique().sort().head(6).to_list()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for cohort in sample_cohorts:\n",
    "    cohort_data = cohort_analysis.filter(pl.col('cohort_month') == cohort)\n",
    "    plt.plot(\n",
    "        cohort_data['months_since_first'].to_list(),\n",
    "        cohort_data['active_customers'].to_list(),\n",
    "        marker='o',\n",
    "        label=f'Cohort {cohort}'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Months Since First Purchase')\n",
    "plt.ylabel('Active Customers')\n",
    "plt.title('Customer Retention by Cohort')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701babad",
   "metadata": {},
   "source": [
    "### 3.5 Purchase Frequency Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a530c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between customer activity metrics\n",
    "customer_metrics = customer_stats.select([\n",
    "    'num_purchases',\n",
    "    'num_unique_items',\n",
    "    'num_orders',\n",
    "    'days_active'\n",
    "])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = customer_metrics.to_pandas().corr()\n",
    "\n",
    "print(\"\\nCustomer Metrics Correlation Matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a754aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Purchases vs Unique Items\n",
    "axes[0, 0].scatter(\n",
    "    customer_stats['num_purchases'].to_numpy(),\n",
    "    customer_stats['num_unique_items'].to_numpy(),\n",
    "    alpha=0.3, s=20\n",
    ")\n",
    "axes[0, 0].set_xlabel('Number of Purchases')\n",
    "axes[0, 0].set_ylabel('Number of Unique Items')\n",
    "axes[0, 0].set_title('Purchases vs Unique Items')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Purchases vs Orders\n",
    "axes[0, 1].scatter(\n",
    "    customer_stats['num_purchases'].to_numpy(),\n",
    "    customer_stats['num_orders'].to_numpy(),\n",
    "    alpha=0.3, s=20, color='green'\n",
    ")\n",
    "axes[0, 1].set_xlabel('Number of Purchases')\n",
    "axes[0, 1].set_ylabel('Number of Orders')\n",
    "axes[0, 1].set_title('Purchases vs Orders')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Days Active vs Purchases\n",
    "axes[1, 0].scatter(\n",
    "    customer_stats['days_active'].to_numpy(),\n",
    "    customer_stats['num_purchases'].to_numpy(),\n",
    "    alpha=0.3, s=20, color='orange'\n",
    ")\n",
    "axes[1, 0].set_xlabel('Days Active')\n",
    "axes[1, 0].set_ylabel('Number of Purchases')\n",
    "axes[1, 0].set_title('Activity Duration vs Purchases')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Items per Order\n",
    "items_per_order = (customer_stats['num_unique_items'] / customer_stats['num_orders']).to_numpy()\n",
    "axes[1, 1].hist(items_per_order[~np.isnan(items_per_order)], bins=30, edgecolor='black', color='purple')\n",
    "axes[1, 1].set_xlabel('Unique Items per Order')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution: Items per Order')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794556f",
   "metadata": {},
   "source": [
    "## 4. Task 3: Preprocessing Recommendations\n",
    "\n",
    "### 4.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df: pl.DataFrame, column: str) -> dict:\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers = df.filter(\n",
    "        (pl.col(column) < lower_bound) | (pl.col(column) > upper_bound)\n",
    "    ).shape[0]\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'q1': q1,\n",
    "        'q3': q3,\n",
    "        'iqr': iqr,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'num_outliers': outliers,\n",
    "        'outlier_rate': outliers / df.shape[0]\n",
    "    }\n",
    "\n",
    "# Detect outliers in customer metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTLIER DETECTION - Customer Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in ['num_purchases', 'num_unique_items', 'num_orders', 'days_active']:\n",
    "    outlier_info = detect_outliers_iqr(customer_stats, col)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Q1: {outlier_info['q1']:.2f}\")\n",
    "    print(f\"  Q3: {outlier_info['q3']:.2f}\")\n",
    "    print(f\"  IQR: {outlier_info['iqr']:.2f}\")\n",
    "    print(f\"  Bounds: [{outlier_info['lower_bound']:.2f}, {outlier_info['upper_bound']:.2f}]\")\n",
    "    print(f\"  Outliers: {outlier_info['num_outliers']} ({outlier_info['outlier_rate']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1896a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers with box plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "metrics = ['num_purchases', 'num_unique_items', 'num_orders', 'days_active']\n",
    "titles = ['Purchases', 'Unique Items', 'Orders', 'Days Active']\n",
    "\n",
    "for ax, metric, title in zip(axes.flat, metrics, titles):\n",
    "    data = customer_stats[metric].to_numpy()\n",
    "    ax.boxplot([data], vert=False)\n",
    "    ax.set_xlabel(title)\n",
    "    ax.set_title(f'Box Plot: {title} per Customer')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cae71",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d433525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DUPLICATES:\")\n",
    "print(f\"   Transactions: {transactions.shape[0] - transactions.unique().shape[0]} duplicate rows\")\n",
    "print(f\"   Items: {items.shape[0] - items.unique().shape[0]} duplicate rows\")\n",
    "print(f\"   Users: {users.shape[0] - users.unique().shape[0]} duplicate rows\")\n",
    "\n",
    "print(\"\\n2. ID CONSISTENCY:\")\n",
    "txn_customers = set(transactions['customer_id'].unique().to_list())\n",
    "user_ids = set(users['customer_id'].unique().to_list())\n",
    "txn_items = set(transactions['item_id'].unique().to_list())\n",
    "item_ids = set(items['item_id'].unique().to_list())\n",
    "\n",
    "print(f\"   Customers in txns not in users: {len(txn_customers - user_ids)}\")\n",
    "print(f\"   Items in txns not in items table: {len(txn_items - item_ids)}\")\n",
    "\n",
    "print(\"\\n3. TEMPORAL CONSISTENCY:\")\n",
    "if 'date_of_birth' in users.columns:\n",
    "    future_dob = users.filter(pl.col('date_of_birth') > pl.lit(datetime.now().date())).shape[0]\n",
    "    print(f\"   Users with future date_of_birth: {future_dob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b436196",
   "metadata": {},
   "source": [
    "### 4.3 Preprocessing Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ce9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = \"\"\"\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "                   PREPROCESSING RECOMMENDATIONS\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "1. COLUMNS TO DROP/HANDLE:\n",
    "   ✓ Check null rates above 50% - consider dropping or imputing\n",
    "   ✓ If 'order_id' has high nulls but not needed, can drop\n",
    "   ✓ Redundant ID columns should be removed after joining\n",
    "\n",
    "2. NULL HANDLING PLAN:\n",
    "   Transactions:\n",
    "   - customer_id, item_id, created_at: DROP rows (critical fields)\n",
    "   - order_id: Fill with auto-generated ID or keep null if not used\n",
    "   \n",
    "   Items:\n",
    "   - item_id: DROP rows (primary key)\n",
    "   - brand/category/age_group: Fill with 'Unknown' or mode\n",
    "   \n",
    "   Users:\n",
    "   - customer_id: DROP rows (primary key)\n",
    "   - date_of_birth: Keep null or impute with median/mode\n",
    "\n",
    "3. OUTLIER HANDLING:\n",
    "   Customer Metrics:\n",
    "   - Cap extreme purchase counts at 99th percentile\n",
    "   - Consider log transformation for skewed distributions\n",
    "   - Flag power users (top 1%) for separate analysis\n",
    "   \n",
    "   Item Metrics:\n",
    "   - Handle cold-start items (< 5 purchases) separately\n",
    "   - Consider minimum support thresholds\n",
    "\n",
    "4. FEATURE ENGINEERING SUGGESTIONS:\n",
    "   Time-based:\n",
    "   - Recency: days since last purchase\n",
    "   - Frequency: purchases per active day\n",
    "   - Seasonality: month, day of week, holidays\n",
    "   \n",
    "   Interaction:\n",
    "   - Purchase velocity: purchases / days_active\n",
    "   - Category diversity: unique categories purchased\n",
    "   - Repeat purchase rate: items bought multiple times\n",
    "   \n",
    "   Categorical:\n",
    "   - One-hot encode brand, category, age_group\n",
    "   - Or use target encoding for high cardinality\n",
    "\n",
    "5. DATA SPLITS:\n",
    "   Recommendation: Time-based split (not random!)\n",
    "   - Training: transactions before date T\n",
    "   - Validation: T to T+14 days\n",
    "   - Test: T+14 to T+30 days\n",
    "   \n",
    "   Prevents data leakage and mimics production scenario\n",
    "\n",
    "6. COLD START HANDLING:\n",
    "   - New users: Use popularity-based recommendations\n",
    "   - New items: Require minimum interaction threshold\n",
    "   - Consider hybrid approach with content-based features\n",
    "\n",
    "7. SPARSITY MITIGATION:\n",
    "   Given high sparsity (>99%):\n",
    "   - Use negative sampling for training\n",
    "   - Consider matrix factorization or embedding methods\n",
    "   - Implement candidate generation stage (top-K retrieval)\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12d642",
   "metadata": {},
   "source": [
    "### 4.4 Quick Preprocessing Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transactions(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Example preprocessing pipeline for transactions.\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        # Remove nulls in critical columns\n",
    "        .filter(\n",
    "            pl.col('customer_id').is_not_null() &\n",
    "            pl.col('item_id').is_not_null() &\n",
    "            pl.col('created_at').is_not_null()\n",
    "        )\n",
    "        # Remove duplicates\n",
    "        .unique()\n",
    "        # Sort by time\n",
    "        .sort('created_at')\n",
    "    )\n",
    "\n",
    "def preprocess_items(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Example preprocessing pipeline for items.\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .filter(pl.col('item_id').is_not_null())\n",
    "        # Fill categorical nulls\n",
    "        .with_columns([\n",
    "            pl.col('brand').fill_null('Unknown'),\n",
    "            pl.col('category').fill_null('Unknown'),\n",
    "            pl.col('age_group').fill_null('Unknown'),\n",
    "        ])\n",
    "        .unique(subset=['item_id'])\n",
    "    )\n",
    "\n",
    "def preprocess_users(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Example preprocessing pipeline for users.\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .filter(pl.col('customer_id').is_not_null())\n",
    "        .unique(subset=['customer_id'])\n",
    "    )\n",
    "\n",
    "print(\"Preprocessing pipeline functions defined.\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  clean_txns = preprocess_transactions(transactions)\")\n",
    "print(\"  clean_items = preprocess_items(items)\")\n",
    "print(\"  clean_users = preprocess_users(users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13346b2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This EDA notebook covered:\n",
    "- **Task 1**: Univariate analysis (null rates, distributions, top items/customers)\n",
    "- **Task 2**: Multivariate analysis (sparsity, correlations, cohort retention)\n",
    "- **Task 3**: Preprocessing recommendations (outliers, null handling, feature engineering)\n",
    "\n",
    "Next steps:\n",
    "1. Apply preprocessing pipeline\n",
    "2. Implement time-based train/test split\n",
    "3. Generate candidates using methods from `candidates.py`\n",
    "4. Build features using `build_feature_label_table()`\n",
    "5. Train and evaluate recommender model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
